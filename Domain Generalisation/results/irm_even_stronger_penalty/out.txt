Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 100, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_even_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 100
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3787326813 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0939589421 
0.8364856620  0.8019559902  0.7995735608  0.7606837607  0.9371257485  0.9041916168  0.6103689567  0.6012738854  14.371257485  0.4529195653  8.1390233040  0.0822901678  0.0041631206  600           0.0935022147 
0.7211714460  0.7261613692  0.7910447761  0.7884615385  0.7986526946  0.7305389222  0.6332697201  0.6433121019  21.556886227  -13.96678027  8.1390233040  1.4180853146  -0.153848656  900           0.0937528062 
0.5216595485  0.5501222494  0.8363539446  0.8012820513  0.4738023952  0.5000000000  0.4875954198  0.4522292994  28.742514970  -56.68304686  8.1390233040  2.2477394044  -0.589307869  1200          0.0941129788 
0.5454545455  0.5745721271  0.8256929638  0.8183760684  0.7507485030  0.7155688623  0.2569974555  0.2407643312  35.928143712  -59.33020267  8.1390233040  2.3291109520  -0.616593136  1500          0.0940634211 
0.4612568639  0.5061124694  0.8448827292  0.8055555556  0.5179640719  0.5179640719  0.2544529262  0.2343949045  43.113772455  -36.47712758  8.1390233040  2.0347538857  -0.385118812  1800          0.0941653093 
0.4972544234  0.5476772616  0.6961620469  0.6346153846  0.5449101796  0.5419161677  0.4688295165  0.4687898089  50.299401197  -28.88329039  8.1390233040  1.9190698512  -0.308023600  2100          0.0943105396 
0.3978035387  0.4547677262  0.5991471215  0.5470085470  0.6002994012  0.5568862275  0.3428753181  0.3668789809  57.485029940  -14.41773042  8.1390233040  2.0186487929  -0.164363799  2400          0.0943830434 
0.7126296522  0.7041564792  0.7622601279  0.7136752137  0.8570359281  0.7964071856  0.3823155216  0.3643312102  64.670658682  -44.18030029  8.1390233040  2.2926832441  -0.464729836  2700          0.0944006387 
0.7071384991  0.7555012225  0.8379530917  0.8183760684  0.8555389222  0.8143712575  0.5314885496  0.5426751592  71.856287425  -49.65922636  8.1390233040  2.1198617830  -0.517790884  3000          0.0950939782 
0.6388041489  0.6650366748  0.8267590618  0.7756410256  0.8278443114  0.7694610778  0.5372137405  0.5554140127  79.041916167  -74.29339531  8.1390233040  2.5307419417  -0.768241375  3300          0.0940923921 
0.7547284930  0.7506112469  0.8678038380  0.8205128205  0.8488023952  0.8083832335  0.6491730280  0.6649681529  86.227544910  -95.51503682  8.1390233040  2.7619042653  -0.982769420  3600          0.0951833447 
0.7504575961  0.7603911980  0.8336886994  0.8076923077  0.8697604790  0.8173652695  0.5524809160  0.5668789809  93.413173652  -70.30653072  8.1390233040  2.6472421871  -0.729537752  3900          0.0948662798 
0.7022574741  0.7090464548  0.8267590618  0.7905982906  0.7252994012  0.6856287425  0.4131679389  0.4038216561  100.59880239  -50.12600257  8.1390233040  2.3383591020  -0.524643614  4200          0.0947494388 
0.6119585113  0.6039119804  0.7777185501  0.7542735043  0.6953592814  0.6766467066  0.3721374046  0.3745222930  107.78443113  -35.00198434  8.1390233040  2.0799121298  -0.370818964  4500          0.0948469392 
0.6308724832  0.6405867971  0.6050106610  0.5769230769  0.7275449102  0.6586826347  0.2057888041  0.2178343949  114.97005988  -2.207576399  8.1390233040  1.8208311947  -0.040284076  4800          0.0947040319 
0.5704697987  0.5819070905  0.5831556503  0.5555555556  0.7694610778  0.7005988024  0.3183842239  0.3426751592  119.76047904  -49.89124073  8.1390233040  2.3570684242  -0.522483093  5000          0.0957515073 
