Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 100.0, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_default
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3747041225 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0937796497 
0.8364856620  0.8019559902  0.7995735608  0.7606837607  0.9371257485  0.9041916168  0.6103689567  0.6012738854  14.371257485  0.4529195653  8.1390233040  0.0822901678  0.0041631206  600           0.0940159210 
0.7211714460  0.7261613692  0.7910447761  0.7884615385  0.7986526946  0.7305389222  0.6332697201  0.6433121019  21.556886227  -13.96678027  8.1390233040  1.4180853146  -0.153848656  900           0.0944469158 
0.5216595485  0.5501222494  0.8363539446  0.8012820513  0.4738023952  0.5000000000  0.4875954198  0.4522292994  28.742514970  -56.68304686  8.1390233040  2.2477394044  -0.589307869  1200          0.0957244245 
0.5454545455  0.5745721271  0.8256929638  0.8183760684  0.7507485030  0.7155688623  0.2569974555  0.2407643312  35.928143712  -59.33020267  8.1390233040  2.3291109520  -0.616593136  1500          0.0941854517 
0.4612568639  0.5061124694  0.8448827292  0.8055555556  0.5179640719  0.5179640719  0.2544529262  0.2343949045  43.113772455  -36.47712758  8.1390233040  2.0347538857  -0.385118812  1800          0.0945545967 
0.4972544234  0.5476772616  0.6961620469  0.6346153846  0.5449101796  0.5419161677  0.4688295165  0.4687898089  50.299401197  -28.88329039  8.1390233040  1.9190698512  -0.308023600  2100          0.0938370315 
0.3978035387  0.4547677262  0.5991471215  0.5470085470  0.6002994012  0.5568862275  0.3428753181  0.3668789809  57.485029940  -14.41773042  8.1390233040  2.0186487929  -0.164363799  2400          0.0941136297 
0.7126296522  0.7041564792  0.7622601279  0.7136752137  0.8570359281  0.7964071856  0.3823155216  0.3643312102  64.670658682  -44.18030029  8.1390233040  2.2926832441  -0.464729836  2700          0.0948632360 
0.7071384991  0.7555012225  0.8379530917  0.8183760684  0.8555389222  0.8143712575  0.5314885496  0.5426751592  71.856287425  -49.65922636  8.1390233040  2.1198617830  -0.517790884  3000          0.0944053769 
0.6388041489  0.6650366748  0.8267590618  0.7756410256  0.8278443114  0.7694610778  0.5372137405  0.5554140127  79.041916167  -74.29339531  8.1390233040  2.5307419417  -0.768241375  3300          0.0946586005 
0.7547284930  0.7506112469  0.8678038380  0.8205128205  0.8488023952  0.8083832335  0.6491730280  0.6649681529  86.227544910  -95.51503682  8.1390233040  2.7619042653  -0.982769420  3600          0.0941479580 
0.7504575961  0.7603911980  0.8336886994  0.8076923077  0.8697604790  0.8173652695  0.5524809160  0.5668789809  93.413173652  -70.30653072  8.1390233040  2.6472421871  -0.729537752  3900          0.0947898809 
0.7022574741  0.7090464548  0.8267590618  0.7905982906  0.7252994012  0.6856287425  0.4131679389  0.4038216561  100.59880239  -50.12600257  8.1390233040  2.3383591020  -0.524643614  4200          0.0944468315 
0.6119585113  0.6039119804  0.7777185501  0.7542735043  0.6953592814  0.6766467066  0.3721374046  0.3745222930  107.78443113  -35.00198434  8.1390233040  2.0799121298  -0.370818964  4500          0.0944624869 
0.6308724832  0.6405867971  0.6050106610  0.5769230769  0.7275449102  0.6586826347  0.2057888041  0.2178343949  114.97005988  -2.207576399  8.1390233040  1.8208311947  -0.040284076  4800          0.0950132847 
0.5704697987  0.5819070905  0.5831556503  0.5555555556  0.7694610778  0.7005988024  0.3183842239  0.3426751592  119.76047904  -49.89124073  8.1390233040  2.3570684242  -0.522483093  5000          0.0947533798 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 25, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_default
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 25
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3622117043 
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/root/IbsATML/PA2/Domain Generalisation/code/domainbed/domainbed/scripts/train.py", line 209, in <module>
    for x,y in next(train_minibatches_iterator)]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 10, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_default
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 10
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.4005048275 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0940776928 
0.9701037218  0.9437652812  0.9578891258  0.9102564103  0.9932634731  0.9640718563  0.7318702290  0.7464968153  14.371257485  0.0401038473  8.1390233040  0.0386401018  0.0005616713  600           0.0940202705 
0.9286150092  0.8997555012  0.9008528785  0.8504273504  0.9797904192  0.9760479042  0.6075063613  0.5821656051  21.556886227  0.1802345057  8.1390233040  0.1470690926  0.0033165412  900           0.0936089206 
0.9566809030  0.9242053790  0.9674840085  0.9423076923  0.9902694611  0.9491017964  0.7121501272  0.7375796178  28.742514970  0.1465720464  8.1390233040  0.1420254308  0.0004546616  1200          0.0945987399 
0.9609517999  0.9144254279  0.9733475480  0.9444444444  0.9940119760  0.9610778443  0.8196564885  0.8458598726  35.928143712  0.1062808765  8.1390233040  0.0986764830  0.0007604393  1500          0.0950982213 
0.9566809030  0.8997555012  0.9765458422  0.9508547009  0.9917664671  0.9520958084  0.7636768448  0.8012738854  43.113772455  0.1058595217  8.1390233040  0.1007039159  0.0005155607  1800          0.0951857837 
0.9792556437  0.9388753056  0.9920042644  0.9572649573  0.9992514970  0.9820359281  0.7480916031  0.7834394904  50.299401197  0.0856069358  8.1390233040  0.0793715742  0.0006235362  2100          0.0945187235 
0.9816961562  0.9413202934  0.9946695096  0.9679487179  1.0000000000  0.9760479042  0.7477735369  0.7910828025  57.485029940  0.0452593150  8.1390233040  0.0505383595  -0.000527904  2400          0.0943888752 
0.9877974375  0.9413202934  0.9861407249  0.9465811966  0.9940119760  0.9580838323  0.7420483461  0.7503184713  64.670658682  0.0455831381  8.1390233040  0.0418087646  0.0003774373  2700          0.0944318159 
0.9835265406  0.9315403423  0.9962686567  0.9594017094  0.9992514970  0.9730538922  0.6968829517  0.7184713376  71.856287425  0.0489669082  8.1390233040  0.0397253616  0.0009241546  3000          0.0948006741 
0.9969493594  0.9559902200  0.9973347548  0.9636752137  0.9992514970  0.9790419162  0.7471374046  0.7694267516  79.041916167  0.0173994388  8.1390233040  0.0186725616  -0.000127312  3300          0.0944923814 
0.9975594875  0.9486552567  0.9989339019  0.9700854701  0.9985029940  0.9700598802  0.7767175573  0.7821656051  86.227544910  0.0096170594  8.1390233040  0.0089483055  0.0000668754  3600          0.0949924827 
0.9993898719  0.9535452323  0.9989339019  0.9679487179  1.0000000000  0.9820359281  0.7700381679  0.7834394904  93.413173652  0.0079873352  8.1390233040  0.0075690344  0.0000418301  3900          0.0950177566 
0.9993898719  0.9559902200  1.0000000000  0.9722222222  1.0000000000  0.9730538922  0.7818066158  0.8089171975  100.59880239  0.0042191009  8.1390233040  0.0041554501  0.0000063651  4200          0.0947714416 
1.0000000000  0.9511002445  0.9994669510  0.9615384615  1.0000000000  0.9850299401  0.7926208651  0.8063694268  107.78443113  0.0039181561  8.1390233040  0.0038980548  0.0000020101  4500          0.0948586051 
1.0000000000  0.9657701711  0.9994669510  0.9658119658  1.0000000000  0.9850299401  0.7862595420  0.7885350318  114.97005988  0.0027809035  8.1390233040  0.0026566531  0.0000124250  4800          0.0948423505 
0.9993898719  0.9608801956  0.9994669510  0.9700854701  1.0000000000  0.9730538922  0.7840330789  0.7885350318  119.76047904  0.0031472954  8.1390233040  0.0030488493  0.0000098446  5000          0.0949185181 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 10, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_default
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 10
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.4021987915 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0937962492 
0.9701037218  0.9437652812  0.9578891258  0.9102564103  0.9932634731  0.9640718563  0.7318702290  0.7464968153  14.371257485  0.0401038473  8.1390233040  0.0386401018  0.0005616713  600           0.0939696956 
0.9286150092  0.8997555012  0.9008528785  0.8504273504  0.9797904192  0.9760479042  0.6075063613  0.5821656051  21.556886227  0.1802345057  8.1390233040  0.1470690926  0.0033165412  900           0.0942721955 
0.9566809030  0.9242053790  0.9674840085  0.9423076923  0.9902694611  0.9491017964  0.7121501272  0.7375796178  28.742514970  0.1465720464  8.1390233040  0.1420254308  0.0004546616  1200          0.0949703447 
0.9609517999  0.9144254279  0.9733475480  0.9444444444  0.9940119760  0.9610778443  0.8196564885  0.8458598726  35.928143712  0.1062808765  8.1390233040  0.0986764830  0.0007604393  1500          0.0966850845 
0.9566809030  0.8997555012  0.9765458422  0.9508547009  0.9917664671  0.9520958084  0.7636768448  0.8012738854  43.113772455  0.1058595217  8.1390233040  0.1007039159  0.0005155607  1800          0.0969660576 
0.9792556437  0.9388753056  0.9920042644  0.9572649573  0.9992514970  0.9820359281  0.7480916031  0.7834394904  50.299401197  0.0856069358  8.1390233040  0.0793715742  0.0006235362  2100          0.0950860826 
0.9816961562  0.9413202934  0.9946695096  0.9679487179  1.0000000000  0.9760479042  0.7477735369  0.7910828025  57.485029940  0.0452593150  8.1390233040  0.0505383595  -0.000527904  2400          0.0959584427 
0.9877974375  0.9413202934  0.9861407249  0.9465811966  0.9940119760  0.9580838323  0.7420483461  0.7503184713  64.670658682  0.0455831381  8.1390233040  0.0418087646  0.0003774373  2700          0.0947309645 
0.9835265406  0.9315403423  0.9962686567  0.9594017094  0.9992514970  0.9730538922  0.6968829517  0.7184713376  71.856287425  0.0489669082  8.1390233040  0.0397253616  0.0009241546  3000          0.0948173285 
0.9969493594  0.9559902200  0.9973347548  0.9636752137  0.9992514970  0.9790419162  0.7471374046  0.7694267516  79.041916167  0.0173994388  8.1390233040  0.0186725616  -0.000127312  3300          0.0949047605 
0.9975594875  0.9486552567  0.9989339019  0.9700854701  0.9985029940  0.9700598802  0.7767175573  0.7821656051  86.227544910  0.0096170594  8.1390233040  0.0089483055  0.0000668754  3600          0.0947530278 
0.9993898719  0.9535452323  0.9989339019  0.9679487179  1.0000000000  0.9820359281  0.7700381679  0.7834394904  93.413173652  0.0079873352  8.1390233040  0.0075690344  0.0000418301  3900          0.0947836431 
0.9993898719  0.9559902200  1.0000000000  0.9722222222  1.0000000000  0.9730538922  0.7818066158  0.8089171975  100.59880239  0.0042191009  8.1390233040  0.0041554501  0.0000063651  4200          0.0951071040 
1.0000000000  0.9511002445  0.9994669510  0.9615384615  1.0000000000  0.9850299401  0.7926208651  0.8063694268  107.78443113  0.0039181561  8.1390233040  0.0038980548  0.0000020101  4500          0.0948496342 
1.0000000000  0.9657701711  0.9994669510  0.9658119658  1.0000000000  0.9850299401  0.7862595420  0.7885350318  114.97005988  0.0027809035  8.1390233040  0.0026566531  0.0000124250  4800          0.0962307946 
0.9993898719  0.9608801956  0.9994669510  0.9700854701  1.0000000000  0.9730538922  0.7840330789  0.7885350318  119.76047904  0.0031472954  8.1390233040  0.0030488493  0.0000098446  5000          0.0960950184 
