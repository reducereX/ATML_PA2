Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 1000.0, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 1000.0
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3537504673 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0940318831 
0.8053691275  0.8141809291  0.6886993603  0.6794871795  0.7754491018  0.7814371257  0.3037531807  0.3197452229  14.371257485  2.3892382437  8.1390233040  0.1369022419  0.0027133155  600           0.0943210840 
0.4453935326  0.4449877751  0.5453091684  0.4978632479  0.5067365269  0.4760479042  0.1641221374  0.1872611465  21.556886227  10.015793580  8.1390233040  1.4100490691  0.0086057445  900           0.0941840347 
0.6979865772  0.6723716381  0.6972281450  0.6602564103  0.6264970060  0.6167664671  0.3559160305  0.3528662420  28.742514970  -3.780626656  8.1390233040  1.6924613166  -0.005473089  1200          0.0943790531 
0.6333129957  0.5843520782  0.5980810235  0.5747863248  0.7851796407  0.7844311377  0.1116412214  0.1108280255  35.928143712  -193.3132791  8.1390233040  1.8656947287  -0.195178972  1500          0.0938761775 
0.7144600366  0.6650366748  0.5692963753  0.5277777778  0.8083832335  0.7724550898  0.4974554707  0.5133757962  43.113772455  -37.16908847  8.1390233040  1.7030321217  -0.038872121  1800          0.0941226196 
0.6284319707  0.5990220049  0.5655650320  0.5726495726  0.8233532934  0.8173652695  0.4942748092  0.5375796178  50.299401197  -220.0976620  8.1390233040  1.8524492327  -0.221950111  2100          0.0943729885 
0.5765710799  0.5330073350  0.4712153518  0.4444444444  0.7500000000  0.7245508982  0.3874045802  0.4101910828  57.485029940  -59.66994474  8.1390233040  1.7958906162  -0.061465833  2400          0.0943614785 
0.6760219646  0.7237163814  0.6428571429  0.6602564103  0.8031437126  0.7904191617  0.5623409669  0.5707006369  64.670658682  -95.18196862  8.1390233040  1.8179251480  -0.096999894  2700          0.0943229437 
0.6754118365  0.6943765281  0.6839019190  0.7222222222  0.7395209581  0.7185628743  0.5715648855  0.6050955414  71.856287425  -133.0021393  8.1390233040  1.8095665930  -0.134811705  3000          0.0944056892 
0.7034777303  0.6845965770  0.6823027719  0.6388888889  0.7417664671  0.6946107784  0.4742366412  0.4840764331  79.041916167  -237.3753295  8.1390233040  1.9727323945  -0.239348061  3300          0.0943865951 
0.6247712020  0.6161369193  0.6636460554  0.6239316239  0.7282934132  0.6886227545  0.2808524173  0.2738853503  86.227544910  -89.31617581  8.1390233040  1.8694513667  -0.091185628  3600          0.0945690346 
0.5155582672  0.5232273839  0.4013859275  0.3717948718  0.7372754491  0.6916167665  0.1816157761  0.2050955414  93.413173652  -303.9269606  8.1390233040  2.1636436808  -0.306090606  3900          0.0948713875 
0.5381330079  0.5647921760  0.5724946695  0.5769230769  0.6556886228  0.5808383234  0.1895674300  0.1694267516  100.59880239  -188.4589695  8.1390233040  2.1656720873  -0.190624640  4200          0.0952563357 
0.5716900549  0.5476772616  0.6865671642  0.6623931624  0.6541916168  0.6287425150  0.2627226463  0.2267515924  107.78443113  -237.1855264  8.1390233040  2.1421252724  -0.239327652  4500          0.0953912457 
0.6082977425  0.6039119804  0.7308102345  0.6965811966  0.7260479042  0.6856287425  0.2916666667  0.2738853503  114.97005988  -406.5797938  8.1390233040  2.2356682193  -0.408815462  4800          0.0956834284 
0.5729103112  0.5672371638  0.6327292111  0.6068376068  0.6953592814  0.6676646707  0.2022900763  0.2178343949  119.76047904  -298.3736445  8.1390233040  2.0793634009  -0.300453007  5000          0.0951651311 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 25, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 25
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3596222401 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0943154621 
0.9640024405  0.9144254279  0.9493603412  0.9059829060  0.9865269461  0.9610778443  0.6758905852  0.6904458599  14.371257485  0.1064490070  8.1390233040  0.0646506229  0.0021149186  600           0.0936360097 
0.7602196461  0.7188264059  0.6476545842  0.6025641026  0.8136227545  0.7514970060  0.4624681934  0.4878980892  21.556886227  -2.136413150  8.1390233040  1.3651420475  -0.140062212  900           0.0943203076 
0.6558877364  0.6332518337  0.7579957356  0.7564102564  0.7642215569  0.7604790419  0.2108778626  0.2229299363  28.742514970  -8.894750553  8.1390233040  2.1037873693  -0.439941516  1200          0.0941572181 
0.5546064674  0.5476772616  0.7457356077  0.7094017094  0.8248502994  0.7904191617  0.3797709924  0.3898089172  35.928143712  -2.978647657  8.1390233040  1.8122240104  -0.191634869  1500          0.0950128961 
0.5997559487  0.6552567237  0.7601279318  0.7606837607  0.7791916168  0.7485029940  0.2888040712  0.3171974522  43.113772455  -2.682592541  8.1390233040  1.8046597634  -0.179490092  1800          0.0943974010 
0.7156802929  0.7261613692  0.8107675906  0.7948717949  0.8143712575  0.7934131737  0.4036259542  0.4216560510  50.299401197  -5.215950786  8.1390233040  1.8884030006  -0.284174150  2100          0.0943564169 
0.6809029896  0.6503667482  0.6295309168  0.6217948718  0.7724550898  0.7544910180  0.5054071247  0.5324840764  57.485029940  -3.269423261  8.1390233040  1.7315724899  -0.200039825  2400          0.0944317396 
0.7248322148  0.7017114914  0.5719616205  0.5769230769  0.8360778443  0.7844311377  0.3963104326  0.4127388535  64.670658682  -8.316758580  8.1390233040  2.0203459839  -0.413484186  2700          0.0946161191 
0.6931055522  0.6772616137  0.7563965885  0.7371794872  0.7911676647  0.7215568862  0.0814249364  0.0866242038  71.856287425  -8.551333123  8.1390233040  2.1629035880  -0.428569471  3000          0.0944929298 
0.6113483832  0.6234718826  0.7062899787  0.6581196581  0.7133233533  0.6706586826  0.2181933842  0.2471337580  79.041916167  -2.360757904  8.1390233040  2.0249965721  -0.175430179  3300          0.0943500845 
0.5649786455  0.5770171149  0.9019189765  0.8589743590  0.6856287425  0.6377245509  0.3730916031  0.3681528662  86.227544910  -7.900714660  8.1390233040  2.0224661835  -0.396927229  3600          0.0944920564 
0.6284319707  0.6234718826  0.7547974414  0.7478632479  0.7791916168  0.7514970060  0.3594147583  0.3363057325  93.413173652  -8.267014676  8.1390233040  2.2515900618  -0.420744191  3900          0.0944501249 
0.6717510677  0.6552567237  0.7052238806  0.7008547009  0.7896706587  0.7514970060  0.3571882952  0.3541401274  100.59880239  -13.29931690  8.1390233040  2.4695181207  -0.630753397  4200          0.0947672319 
0.7169005491  0.6577017115  0.5026652452  0.4615384615  0.8555389222  0.7694610778  0.2350508906  0.2585987261  107.78443113  -20.43074037  8.1390233040  2.6539698178  -0.923388410  4500          0.0946814164 
0.7303233679  0.7041564792  0.6609808102  0.6495726496  0.8884730539  0.8173652695  0.3311068702  0.3464968153  114.97005988  -19.82959846  8.1390233040  2.5842687408  -0.896554688  4800          0.0947254197 
0.5698596705  0.5574572127  0.5021321962  0.4957264957  0.7252994012  0.6766467066  0.1962468193  0.1961783439  119.76047904  -12.43237737  8.1390233040  2.6434470254  -0.603032976  5000          0.0945629466 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 25, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 25
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3601408005 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0936934153 
0.9640024405  0.9144254279  0.9493603412  0.9059829060  0.9865269461  0.9610778443  0.6758905852  0.6904458599  14.371257485  0.1064490070  8.1390233040  0.0646506229  0.0021149186  600           0.0942846648 
0.7602196461  0.7188264059  0.6476545842  0.6025641026  0.8136227545  0.7514970060  0.4624681934  0.4878980892  21.556886227  -2.136413150  8.1390233040  1.3651420475  -0.140062212  900           0.0947395674 
0.6558877364  0.6332518337  0.7579957356  0.7564102564  0.7642215569  0.7604790419  0.2108778626  0.2229299363  28.742514970  -8.894750553  8.1390233040  2.1037873693  -0.439941516  1200          0.0941767208 
0.5546064674  0.5476772616  0.7457356077  0.7094017094  0.8248502994  0.7904191617  0.3797709924  0.3898089172  35.928143712  -2.978647657  8.1390233040  1.8122240104  -0.191634869  1500          0.0943519346 
0.5997559487  0.6552567237  0.7601279318  0.7606837607  0.7791916168  0.7485029940  0.2888040712  0.3171974522  43.113772455  -2.682592541  8.1390233040  1.8046597634  -0.179490092  1800          0.0943630608 
0.7156802929  0.7261613692  0.8107675906  0.7948717949  0.8143712575  0.7934131737  0.4036259542  0.4216560510  50.299401197  -5.215950786  8.1390233040  1.8884030006  -0.284174150  2100          0.0944768699 
0.6809029896  0.6503667482  0.6295309168  0.6217948718  0.7724550898  0.7544910180  0.5054071247  0.5324840764  57.485029940  -3.269423261  8.1390233040  1.7315724899  -0.200039825  2400          0.0943768469 
0.7248322148  0.7017114914  0.5719616205  0.5769230769  0.8360778443  0.7844311377  0.3963104326  0.4127388535  64.670658682  -8.316758580  8.1390233040  2.0203459839  -0.413484186  2700          0.0948274414 
0.6931055522  0.6772616137  0.7563965885  0.7371794872  0.7911676647  0.7215568862  0.0814249364  0.0866242038  71.856287425  -8.551333123  8.1390233040  2.1629035880  -0.428569471  3000          0.0945835233 
0.6113483832  0.6234718826  0.7062899787  0.6581196581  0.7133233533  0.6706586826  0.2181933842  0.2471337580  79.041916167  -2.360757904  8.1390233040  2.0249965721  -0.175430179  3300          0.0945785817 
0.5649786455  0.5770171149  0.9019189765  0.8589743590  0.6856287425  0.6377245509  0.3730916031  0.3681528662  86.227544910  -7.900714660  8.1390233040  2.0224661835  -0.396927229  3600          0.0950548983 
0.6284319707  0.6234718826  0.7547974414  0.7478632479  0.7791916168  0.7514970060  0.3594147583  0.3363057325  93.413173652  -8.267014676  8.1390233040  2.2515900618  -0.420744191  3900          0.0948799086 
0.6717510677  0.6552567237  0.7052238806  0.7008547009  0.7896706587  0.7514970060  0.3571882952  0.3541401274  100.59880239  -13.29931690  8.1390233040  2.4695181207  -0.630753397  4200          0.0949397628 
0.7169005491  0.6577017115  0.5026652452  0.4615384615  0.8555389222  0.7694610778  0.2350508906  0.2585987261  107.78443113  -20.43074037  8.1390233040  2.6539698178  -0.923388410  4500          0.0948295665 
0.7303233679  0.7041564792  0.6609808102  0.6495726496  0.8884730539  0.8173652695  0.3311068702  0.3464968153  114.97005988  -19.82959846  8.1390233040  2.5842687408  -0.896554688  4800          0.0949715074 
0.5698596705  0.5574572127  0.5021321962  0.4957264957  0.7252994012  0.6766467066  0.1962468193  0.1961783439  119.76047904  -12.43237737  8.1390233040  2.6434470254  -0.603032976  5000          0.0952624702 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 25, "irm_penalty_anneal_iters": 2000}
	hparams_seed: 0
	output_dir: ./results/irm_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 25
	irm_penalty_anneal_iters: 2000
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3579769135 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0945837069 
0.9945088469  0.9437652812  0.9957356077  0.9444444444  0.9992514970  0.9700598802  0.7258269720  0.7452229299  14.371257485  0.0284110635  8.1374363899  0.0279533535  0.0004577100  600           0.0938411800 
0.9981696156  0.9486552567  0.9978678038  0.9551282051  1.0000000000  0.9790419162  0.7627226463  0.7643312102  21.556886227  0.0195341829  8.1374363899  0.0193819361  0.0001522469  900           0.0941965071 
0.9963392312  0.9462102689  0.9978678038  0.9722222222  0.9985029940  0.9820359281  0.7350508906  0.7541401274  28.742514970  0.0135819275  8.1374363899  0.0135598418  0.0000220857  1200          0.0940234264 
0.9951189750  0.9462102689  0.9989339019  0.9594017094  0.9977544910  0.9760479042  0.7856234097  0.7961783439  35.928143712  0.0120548298  8.1374363899  0.0120207887  0.0000340411  1500          0.0940224195 
0.9993898719  0.9388753056  0.9989339019  0.9679487179  1.0000000000  0.9850299401  0.8311068702  0.8407643312  43.113772455  0.0095136734  8.1374363899  0.0093909924  0.0001226810  1800          0.0948254315 
0.9505796217  0.8801955990  0.9733475480  0.9358974359  0.9453592814  0.8982035928  0.6790712468  0.7197452229  50.299401197  0.0397335021  8.1390233040  0.0340919696  0.0002995087  2100          0.0948171147 
0.8492983527  0.8410757946  0.9461620469  0.9081196581  0.9610778443  0.9401197605  0.7197837150  0.7375796178  57.485029940  0.3927329128  8.1390233040  0.2227824057  0.0067980202  2400          0.0952998535 
0.8517388652  0.8190709046  0.9738805970  0.9316239316  0.9603293413  0.9341317365  0.7391857506  0.7197452229  64.670658682  0.4074345338  8.1390233040  0.3444715065  0.0025185211  2700          0.0953986311 
0.6961561928  0.6454767726  0.6268656716  0.6153846154  0.8330838323  0.7694610778  0.5276717557  0.5070063694  71.856287425  0.8160629240  8.1390233040  0.6748364986  0.0056490568  3000          0.0950517623 
0.6924954240  0.6772616137  0.7739872068  0.7628205128  0.7979041916  0.7485029940  0.4510178117  0.4573248408  79.041916167  0.0460944850  8.1390233040  1.2266833435  -0.047223554  3300          0.0949677316 
0.5930445394  0.5965770171  0.7942430704  0.7841880342  0.6392215569  0.5838323353  0.5982824427  0.6089171975  86.227544910  -2.959899661  8.1390233040  1.8253252546  -0.191408996  3600          0.0950284274 
0.7242220866  0.7066014670  0.7809168443  0.7863247863  0.8046407186  0.7395209581  0.3291984733  0.3070063694  93.413173652  -18.34509226  8.1390233040  2.3337595222  -0.827154073  3900          0.0948685336 
0.7370347773  0.6992665037  0.8480810235  0.8482905983  0.7769461078  0.7125748503  0.6230916031  0.6407643312  100.59880239  -23.94861320  8.1390233040  2.8916931506  -1.073612262  4200          0.0950782283 
0.7449664430  0.7408312958  0.7878464819  0.7521367521  0.7904191617  0.7514970060  0.5540712468  0.5324840764  107.78443113  -2.234800964  8.1390233040  1.9198963124  -0.166187889  4500          0.0956403653 
0.6894447834  0.6845965770  0.7441364606  0.7072649573  0.7514970060  0.6736526946  0.4659669211  0.4942675159  114.97005988  -6.494364261  8.1390233040  2.1402945171  -0.345386352  4800          0.0953520751 
0.7413056742  0.6943765281  0.7265458422  0.7179487179  0.8562874251  0.8053892216  0.3326972010  0.3541401274  119.76047904  -8.299068175  8.1390233040  2.2243602914  -0.420937146  5000          0.0949406278 
Environment:
	Python: 3.12.11
	PyTorch: 2.8.0+cu129
	Torchvision: 0.23.0+cu129
	CUDA: 12.9
	CUDNN: 91002
	NumPy: 2.1.2
	PIL: 11.0.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"progress_bar": true, "irm_lambda": 25, "irm_penalty_anneal_iters": 500}
	hparams_seed: 0
	output_dir: ./results/irm_stronger_penalty
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	irm_lambda: 25
	irm_penalty_anneal_iters: 500
	lars: False
	linear_steps: 500
	lr: 5e-05
	nonlinear_classifier: False
	progress_bar: True
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.0
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.0
	weight_decay: 0.0
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    
0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3839375973 
0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0937460820 
0.9640024405  0.9144254279  0.9493603412  0.9059829060  0.9865269461  0.9610778443  0.6758905852  0.6904458599  14.371257485  0.1064490070  8.1390233040  0.0646506229  0.0021149186  600           0.0935602132 
0.7602196461  0.7188264059  0.6476545842  0.6025641026  0.8136227545  0.7514970060  0.4624681934  0.4878980892  21.556886227  -2.136413150  8.1390233040  1.3651420475  -0.140062212  900           0.0938191263 
0.6558877364  0.6332518337  0.7579957356  0.7564102564  0.7642215569  0.7604790419  0.2108778626  0.2229299363  28.742514970  -8.894750553  8.1390233040  2.1037873693  -0.439941516  1200          0.0946129910 
0.5546064674  0.5476772616  0.7457356077  0.7094017094  0.8248502994  0.7904191617  0.3797709924  0.3898089172  35.928143712  -2.978647657  8.1390233040  1.8122240104  -0.191634869  1500          0.0942125257 
0.5997559487  0.6552567237  0.7601279318  0.7606837607  0.7791916168  0.7485029940  0.2888040712  0.3171974522  43.113772455  -2.682592541  8.1390233040  1.8046597634  -0.179490092  1800          0.0946811040 
0.7156802929  0.7261613692  0.8107675906  0.7948717949  0.8143712575  0.7934131737  0.4036259542  0.4216560510  50.299401197  -5.215950786  8.1390233040  1.8884030006  -0.284174150  2100          0.0964806835 
0.6809029896  0.6503667482  0.6295309168  0.6217948718  0.7724550898  0.7544910180  0.5054071247  0.5324840764  57.485029940  -3.269423261  8.1390233040  1.7315724899  -0.200039825  2400          0.0944827835 
0.7248322148  0.7017114914  0.5719616205  0.5769230769  0.8360778443  0.7844311377  0.3963104326  0.4127388535  64.670658682  -8.316758580  8.1390233040  2.0203459839  -0.413484186  2700          0.0943294334 
0.6931055522  0.6772616137  0.7563965885  0.7371794872  0.7911676647  0.7215568862  0.0814249364  0.0866242038  71.856287425  -8.551333123  8.1390233040  2.1629035880  -0.428569471  3000          0.0949624507 
0.6113483832  0.6234718826  0.7062899787  0.6581196581  0.7133233533  0.6706586826  0.2181933842  0.2471337580  79.041916167  -2.360757904  8.1390233040  2.0249965721  -0.175430179  3300          0.0952030373 
0.5649786455  0.5770171149  0.9019189765  0.8589743590  0.6856287425  0.6377245509  0.3730916031  0.3681528662  86.227544910  -7.900714660  8.1390233040  2.0224661835  -0.396927229  3600          0.0947308079 
0.6284319707  0.6234718826  0.7547974414  0.7478632479  0.7791916168  0.7514970060  0.3594147583  0.3363057325  93.413173652  -8.267014676  8.1390233040  2.2515900618  -0.420744191  3900          0.0945808800 
0.6717510677  0.6552567237  0.7052238806  0.7008547009  0.7896706587  0.7514970060  0.3571882952  0.3541401274  100.59880239  -13.29931690  8.1390233040  2.4695181207  -0.630753397  4200          0.0947346814 
0.7169005491  0.6577017115  0.5026652452  0.4615384615  0.8555389222  0.7694610778  0.2350508906  0.2585987261  107.78443113  -20.43074037  8.1390233040  2.6539698178  -0.923388410  4500          0.0946750855 
0.7303233679  0.7041564792  0.6609808102  0.6495726496  0.8884730539  0.8173652695  0.3311068702  0.3464968153  114.97005988  -19.82959846  8.1390233040  2.5842687408  -0.896554688  4800          0.0946627935 
0.5698596705  0.5574572127  0.5021321962  0.4957264957  0.7252994012  0.6766467066  0.1962468193  0.1961783439  119.76047904  -12.43237737  8.1390233040  2.6434470254  -0.603032976  5000          0.0946760094 
