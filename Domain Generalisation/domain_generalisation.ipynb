{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d80a69e",
      "metadata": {},
      "source": [
        "# üìã Task 2: Domain Generalization via Invariant & Robust Learning\n",
        "\n",
        "In this task, we explore Domain Generalization (DG), where a model is trained on multiple source domains and must generalize to a completely unseen target domain. We will implement and compare four methods: ERM, IRM, GroupDRO, and SAM.\n",
        "\n",
        "Our setup will use the **PACS dataset**. We will train on the **Art, Cartoon, and Photo** domains, holding out the **Sketch** domain as our unseen test environment, as suggested in the assignment manual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Part 1: Empirical Risk Minimization (ERM) Baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.1. Overview**\n",
        "\n",
        "We begin by establishing a baseline using standard **Empirical Risk Minimization (ERM)**. This approach involves merging all data from the source domains into a single dataset and training a standard classifier on it. This model's performance on the unseen target domain will serve as the benchmark against which we will compare more advanced DG techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1.2. Environment Setup**\n",
        "\n",
        "First, we need to set up the Python environment to ensure the notebook can find and import the DomainBed library from our `code/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68feccbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully imported DomainBed.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# The path to the 'domainbed' repository inside the 'code' folder\n",
        "# This is the parent directory of the actual 'domainbed' package\n",
        "module_path = os.path.abspath(os.path.join(\".\", \"code\", \"domainbed\"))\n",
        "\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    print(f\"‚úÖ Added '{module_path}' to Python path.\")\n",
        "\n",
        "# Import the main training function from DomainBed\n",
        "try:\n",
        "    from domainbed.scripts import train\n",
        "\n",
        "    print(\"‚úÖ Successfully imported DomainBed.\")\n",
        "except ImportError as e:\n",
        "    print(\n",
        "        \"‚ùå Error importing DomainBed. Check that the path is correct and the repository is at './code/domainbed'.\"\n",
        "    )\n",
        "    print(e)\n",
        "\n",
        "# Set plotting style for later\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc0ab02",
      "metadata": {},
      "source": [
        "### **1.3. Experiment Runner Function**\n",
        "\n",
        "To keep our code clean, we'll define a helper function that can launch any DomainBed experiment by taking a dictionary of arguments. This function mimics passing arguments via the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f543f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(args_dict):\n",
        "    \"\"\"\n",
        "    Builds a command-line command from a dictionary of arguments\n",
        "    and executes the DomainBed training script, setting the PYTHONPATH.\n",
        "    \"\"\"\n",
        "    # Define the path to the directory containing the 'domainbed' package\n",
        "    module_path = os.path.abspath(os.path.join('.', 'code', 'domainbed'))\n",
        "    \n",
        "    # Enclose the module_path in quotes to handle spaces in the directory name.\n",
        "    command = f'PYTHONPATH=\"{module_path}\" python -m domainbed.scripts.train'\n",
        "    \n",
        "    # Append each argument from the dictionary to the command string\n",
        "    for key, value in args_dict.items():\n",
        "        if isinstance(value, bool) and value:\n",
        "            command += f\" --{key}\"\n",
        "        elif not (isinstance(value, bool) and not value):\n",
        "            command += f\" --{key} {value}\"\n",
        "            \n",
        "    print(\"üöÄ Executing Command:\")\n",
        "    print(command)\n",
        "    \n",
        "    # Execute the command in the shell\n",
        "    os.system(command)\n",
        "    \n",
        "    print(f\"\\nüéâ Training finished for {args_dict.get('algorithm', 'N/A')}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b72331cf",
      "metadata": {},
      "source": [
        "### **1.4. Run ERM Training**\n",
        "\n",
        "Now, we define the specific parameters for our ERM baseline experiment and launch the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6da01a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Executing Command:\n",
            "PYTHONPATH=\"/root/IbsATML/PA2/Domain Generalisation/code/domainbed\" python -m domainbed.scripts.train --data_dir ./data/ --dataset PACS --algorithm ERM --test_env 3 --output_dir ./results/erm --hparams_seed 0 --trial_seed 0 --seed 0 --hparams '{\"progress_bar\": true}'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/venv/main/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment:\n",
            "\tPython: 3.12.11\n",
            "\tPyTorch: 2.8.0+cu129\n",
            "\tTorchvision: 0.23.0+cu129\n",
            "\tCUDA: 12.9\n",
            "\tCUDNN: 91002\n",
            "\tNumPy: 2.1.2\n",
            "\tPIL: 11.0.0\n",
            "Args:\n",
            "\talgorithm: ERM\n",
            "\tcheckpoint_freq: None\n",
            "\tdata_dir: ./data/\n",
            "\tdataset: PACS\n",
            "\tholdout_fraction: 0.2\n",
            "\thparams: {\"progress_bar\": true}\n",
            "\thparams_seed: 0\n",
            "\toutput_dir: ./results/erm\n",
            "\tsave_model_every_checkpoint: False\n",
            "\tseed: 0\n",
            "\tskip_model_save: False\n",
            "\tsteps: None\n",
            "\ttask: domain_generalization\n",
            "\ttest_envs: [3]\n",
            "\ttrial_seed: 0\n",
            "\tuda_holdout_fraction: 0\n",
            "HParams:\n",
            "\tbatch_size: 32\n",
            "\tclass_balanced: False\n",
            "\tdata_augmentation: True\n",
            "\tdinov2: False\n",
            "\tfreeze_bn: False\n",
            "\tlars: False\n",
            "\tlinear_steps: 500\n",
            "\tlr: 5e-05\n",
            "\tnonlinear_classifier: False\n",
            "\tprogress_bar: True\n",
            "\tresnet18: False\n",
            "\tresnet50_augmix: True\n",
            "\tresnet_dropout: 0.0\n",
            "\tvit: False\n",
            "\tvit_attn_tune: False\n",
            "\tvit_dropout: 0.0\n",
            "\tweight_decay: 0.0\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    \n",
            "0.1116534472  0.1173594132  0.1897654584  0.2179487179  0.2050898204  0.2095808383  0.0881043257  0.1070063694  0.0000000000  1.9803797007  7.9325380325  0             0.3455526829 \n",
            "0.9914582062  0.9633251834  0.9882729211  0.9636752137  0.9977544910  0.9880239521  0.7665394402  0.7847133758  7.1856287425  0.2129333733  8.1374368668  300           0.0893903550 \n",
            "0.9938987187  0.9633251834  0.9978678038  0.9594017094  0.9992514970  0.9730538922  0.7099236641  0.7184713376  14.371257485  0.0289216487  8.1374368668  600           0.0894707084 \n",
            "0.9957291031  0.9462102689  0.9930703625  0.9487179487  0.9992514970  0.9820359281  0.7722646310  0.7808917197  21.556886227  0.0178145896  8.1374368668  900           0.0898346472 \n",
            "0.9981696156  0.9535452323  0.9978678038  0.9764957265  1.0000000000  0.9850299401  0.7630407125  0.7770700637  28.742514970  0.0159850566  8.1374368668  1200          0.0904598427 \n",
            "0.9987797437  0.9511002445  0.9989339019  0.9551282051  1.0000000000  0.9850299401  0.7595419847  0.7923566879  35.928143712  0.0091424787  8.1374368668  1500          0.0907076430 \n",
            "0.9945088469  0.9266503667  0.9984008529  0.9700854701  1.0000000000  0.9760479042  0.7786259542  0.7821656051  43.113772455  0.0093627543  8.1374368668  1800          0.0907317742 \n",
            "0.9987797437  0.9535452323  0.9994669510  0.9615384615  1.0000000000  0.9880239521  0.8015267176  0.8216560510  50.299401197  0.0115494104  8.1374368668  2100          0.0909458709 \n",
            "0.9987797437  0.9364303178  0.9994669510  0.9786324786  1.0000000000  0.9820359281  0.7340966921  0.7477707006  57.485029940  0.0079947531  8.1374368668  2400          0.0912309162 \n",
            "0.9993898719  0.9511002445  0.9984008529  0.9594017094  0.9992514970  0.9730538922  0.7283715013  0.7579617834  64.670658682  0.0107372985  8.1374368668  2700          0.0910592182 \n",
            "0.9987797437  0.9339853301  1.0000000000  0.9679487179  0.9985029940  0.9640718563  0.7264631043  0.7414012739  71.856287425  0.0092497070  8.1374368668  3000          0.0908624522 \n",
            "0.9981696156  0.9559902200  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.7722646310  0.8114649682  79.041916167  0.0082652500  8.1374368668  3300          0.0911255137 \n",
            "1.0000000000  0.9633251834  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.7270992366  0.7617834395  86.227544910  0.0035999958  8.1374368668  3600          0.0909262109 \n",
            "0.9969493594  0.9511002445  0.9994669510  0.9658119658  1.0000000000  0.9700598802  0.6914758270  0.7248407643  93.413173652  0.0021751232  8.1374368668  3900          0.0909039990 \n",
            "0.9981696156  0.9608801956  0.9994669510  0.9743589744  1.0000000000  0.9700598802  0.7767175573  0.8076433121  100.59880239  0.0063570482  8.1374368668  4200          0.0912971330 \n",
            "0.9975594875  0.9559902200  1.0000000000  0.9487179487  1.0000000000  0.9790419162  0.7888040712  0.8114649682  107.78443113  0.0101484489  8.1374368668  4500          0.0912606422 \n",
            "0.9975594875  0.9339853301  0.9984008529  0.9636752137  1.0000000000  0.9790419162  0.7942111959  0.8165605096  114.97005988  0.0067233207  8.1374368668  4800          0.0913452784 \n",
            "0.9993898719  0.9486552567  0.9984008529  0.9722222222  1.0000000000  0.9790419162  0.7983460560  0.8382165605  119.76047904  0.0049766661  8.1374368668  5000          0.0910436285 \n",
            "\n",
            "üéâ Training finished for ERM.\n"
          ]
        }
      ],
      "source": [
        "# --- ERM Experiment Configuration ---\n",
        "\n",
        "# 1. Define hyperparameters in their own dictionary.\n",
        "hparams = {\n",
        "    'progress_bar': True\n",
        "}\n",
        "\n",
        "# 2. Configure the main experiment arguments.\n",
        "erm_args = {\n",
        "    'data_dir': './data/',\n",
        "    'dataset': 'PACS',\n",
        "    'algorithm': 'ERM',\n",
        "    'test_env': 3,  # The index for the 'Sketch' domain in PACS\n",
        "    'output_dir': './results/erm',\n",
        "    'hparams_seed': 0,\n",
        "    'trial_seed': 0,\n",
        "    'seed': 0,\n",
        "    'hparams': f\"'{json.dumps(hparams)}'\"\n",
        "}\n",
        "\n",
        "run_experiment(erm_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcae15a6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **Part 2: Invariant Risk Minimization (IRM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c138323",
      "metadata": {},
      "source": [
        "### **2.1. Overview**\n",
        "\n",
        "Now we move to **Invariant Risk Minimization (IRM)**, as required by Part 2 of the assignment. The core idea behind IRM is to learn a feature representation where the optimal classifier is the same across all training domains. This is intended to prevent the model from relying on spurious, domain-specific correlations, thereby improving generalization to unseen domains.\n",
        "\n",
        "We will first run IRM with its default hyperparameters from DomainBed and then perform an ablation study with a stronger penalty weight to analyze its stability and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956719e1",
      "metadata": {},
      "source": [
        "### **2.2. Run IRM Training (Default Hyperparameters)**\n",
        "\n",
        "We start with the default IRM penalty `irm_lambda=25`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6903c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Executing Command:\n",
            "PYTHONPATH=\"/root/IbsATML/PA2/Domain Generalisation/code/domainbed\" python -m domainbed.scripts.train --data_dir ./data/ --dataset PACS --algorithm IRM --test_env 3 --output_dir ./results/irm_default --hparams_seed 0 --trial_seed 0 --seed 0 --hparams '{\"progress_bar\": true, \"irm_lambda\": 10, \"irm_penalty_anneal_iters\": 500}'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/venv/main/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment:\n",
            "\tPython: 3.12.11\n",
            "\tPyTorch: 2.8.0+cu129\n",
            "\tTorchvision: 0.23.0+cu129\n",
            "\tCUDA: 12.9\n",
            "\tCUDNN: 91002\n",
            "\tNumPy: 2.1.2\n",
            "\tPIL: 11.0.0\n",
            "Args:\n",
            "\talgorithm: IRM\n",
            "\tcheckpoint_freq: None\n",
            "\tdata_dir: ./data/\n",
            "\tdataset: PACS\n",
            "\tholdout_fraction: 0.2\n",
            "\thparams: {\"progress_bar\": true, \"irm_lambda\": 10, \"irm_penalty_anneal_iters\": 500}\n",
            "\thparams_seed: 0\n",
            "\toutput_dir: ./results/irm_default\n",
            "\tsave_model_every_checkpoint: False\n",
            "\tseed: 0\n",
            "\tskip_model_save: False\n",
            "\tsteps: None\n",
            "\ttask: domain_generalization\n",
            "\ttest_envs: [3]\n",
            "\ttrial_seed: 0\n",
            "\tuda_holdout_fraction: 0\n",
            "HParams:\n",
            "\tbatch_size: 32\n",
            "\tclass_balanced: False\n",
            "\tdata_augmentation: True\n",
            "\tdinov2: False\n",
            "\tfreeze_bn: False\n",
            "\tirm_lambda: 10\n",
            "\tirm_penalty_anneal_iters: 500\n",
            "\tlars: False\n",
            "\tlinear_steps: 500\n",
            "\tlr: 5e-05\n",
            "\tnonlinear_classifier: False\n",
            "\tprogress_bar: True\n",
            "\tresnet18: False\n",
            "\tresnet50_augmix: True\n",
            "\tresnet_dropout: 0.0\n",
            "\tvit: False\n",
            "\tvit_attn_tune: False\n",
            "\tvit_dropout: 0.0\n",
            "\tweight_decay: 0.0\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    \n",
            "0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.4005048275 \n",
            "0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0940776928 \n",
            "0.9701037218  0.9437652812  0.9578891258  0.9102564103  0.9932634731  0.9640718563  0.7318702290  0.7464968153  14.371257485  0.0401038473  8.1390233040  0.0386401018  0.0005616713  600           0.0940202705 \n",
            "0.9286150092  0.8997555012  0.9008528785  0.8504273504  0.9797904192  0.9760479042  0.6075063613  0.5821656051  21.556886227  0.1802345057  8.1390233040  0.1470690926  0.0033165412  900           0.0936089206 \n",
            "0.9566809030  0.9242053790  0.9674840085  0.9423076923  0.9902694611  0.9491017964  0.7121501272  0.7375796178  28.742514970  0.1465720464  8.1390233040  0.1420254308  0.0004546616  1200          0.0945987399 \n",
            "0.9609517999  0.9144254279  0.9733475480  0.9444444444  0.9940119760  0.9610778443  0.8196564885  0.8458598726  35.928143712  0.1062808765  8.1390233040  0.0986764830  0.0007604393  1500          0.0950982213 \n",
            "0.9566809030  0.8997555012  0.9765458422  0.9508547009  0.9917664671  0.9520958084  0.7636768448  0.8012738854  43.113772455  0.1058595217  8.1390233040  0.1007039159  0.0005155607  1800          0.0951857837 \n",
            "0.9792556437  0.9388753056  0.9920042644  0.9572649573  0.9992514970  0.9820359281  0.7480916031  0.7834394904  50.299401197  0.0856069358  8.1390233040  0.0793715742  0.0006235362  2100          0.0945187235 \n",
            "0.9816961562  0.9413202934  0.9946695096  0.9679487179  1.0000000000  0.9760479042  0.7477735369  0.7910828025  57.485029940  0.0452593150  8.1390233040  0.0505383595  -0.000527904  2400          0.0943888752 \n",
            "0.9877974375  0.9413202934  0.9861407249  0.9465811966  0.9940119760  0.9580838323  0.7420483461  0.7503184713  64.670658682  0.0455831381  8.1390233040  0.0418087646  0.0003774373  2700          0.0944318159 \n",
            "0.9835265406  0.9315403423  0.9962686567  0.9594017094  0.9992514970  0.9730538922  0.6968829517  0.7184713376  71.856287425  0.0489669082  8.1390233040  0.0397253616  0.0009241546  3000          0.0948006741 \n",
            "0.9969493594  0.9559902200  0.9973347548  0.9636752137  0.9992514970  0.9790419162  0.7471374046  0.7694267516  79.041916167  0.0173994388  8.1390233040  0.0186725616  -0.000127312  3300          0.0944923814 \n",
            "0.9975594875  0.9486552567  0.9989339019  0.9700854701  0.9985029940  0.9700598802  0.7767175573  0.7821656051  86.227544910  0.0096170594  8.1390233040  0.0089483055  0.0000668754  3600          0.0949924827 \n",
            "0.9993898719  0.9535452323  0.9989339019  0.9679487179  1.0000000000  0.9820359281  0.7700381679  0.7834394904  93.413173652  0.0079873352  8.1390233040  0.0075690344  0.0000418301  3900          0.0950177566 \n",
            "0.9993898719  0.9559902200  1.0000000000  0.9722222222  1.0000000000  0.9730538922  0.7818066158  0.8089171975  100.59880239  0.0042191009  8.1390233040  0.0041554501  0.0000063651  4200          0.0947714416 \n",
            "1.0000000000  0.9511002445  0.9994669510  0.9615384615  1.0000000000  0.9850299401  0.7926208651  0.8063694268  107.78443113  0.0039181561  8.1390233040  0.0038980548  0.0000020101  4500          0.0948586051 \n",
            "1.0000000000  0.9657701711  0.9994669510  0.9658119658  1.0000000000  0.9850299401  0.7862595420  0.7885350318  114.97005988  0.0027809035  8.1390233040  0.0026566531  0.0000124250  4800          0.0948423505 \n",
            "0.9993898719  0.9608801956  0.9994669510  0.9700854701  1.0000000000  0.9730538922  0.7840330789  0.7885350318  119.76047904  0.0031472954  8.1390233040  0.0030488493  0.0000098446  5000          0.0949185181 \n",
            "\n",
            "üéâ Training finished for IRM.\n"
          ]
        }
      ],
      "source": [
        "# --- IRM Experiment Configuration (Default) ---\n",
        "\n",
        "# 1. Define hyperparameters for the default IRM run.\n",
        "hparams_irm_default = {\n",
        "    \"progress_bar\": True,\n",
        "    \"irm_lambda\": 10,  # Default penalty weight\n",
        "    \"irm_penalty_anneal_iters\": 500,  # Steps to anneal the penalty\n",
        "}\n",
        "\n",
        "# 2. Configure the main experiment arguments.\n",
        "irm_args_default = {\n",
        "    \"data_dir\": \"./data/\",\n",
        "    \"dataset\": \"PACS\",\n",
        "    \"algorithm\": \"IRM\",\n",
        "    \"test_env\": 3,  # Sketch domain\n",
        "    \"output_dir\": \"./results/irm_default\",\n",
        "    \"hparams_seed\": 0,\n",
        "    \"trial_seed\": 0,\n",
        "    \"seed\": 0,\n",
        "    \"hparams\": f\"'{json.dumps(hparams_irm_default)}'\",\n",
        "}\n",
        "\n",
        "run_experiment(irm_args_default)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68df4754",
      "metadata": {},
      "source": [
        "### **2.3. Ablation Study: Run IRM with Stronger Penalty**\n",
        "\n",
        "We will now conduct a stability analysis by increasing the penalty weight to `irm_lambda=25`. This will help us understand how sensitive the IRM algorithm is to this hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73fd6b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Executing Command:\n",
            "PYTHONPATH=\"/root/IbsATML/PA2/Domain Generalisation/code/domainbed\" python -m domainbed.scripts.train --data_dir ./data/ --dataset PACS --algorithm IRM --test_env 3 --output_dir ./results/irm_stronger_penalty --hparams_seed 0 --trial_seed 0 --seed 0 --hparams '{\"progress_bar\": true, \"irm_lambda\": 25, \"irm_penalty_anneal_iters\": 500}'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/venv/main/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment:\n",
            "\tPython: 3.12.11\n",
            "\tPyTorch: 2.8.0+cu129\n",
            "\tTorchvision: 0.23.0+cu129\n",
            "\tCUDA: 12.9\n",
            "\tCUDNN: 91002\n",
            "\tNumPy: 2.1.2\n",
            "\tPIL: 11.0.0\n",
            "Args:\n",
            "\talgorithm: IRM\n",
            "\tcheckpoint_freq: None\n",
            "\tdata_dir: ./data/\n",
            "\tdataset: PACS\n",
            "\tholdout_fraction: 0.2\n",
            "\thparams: {\"progress_bar\": true, \"irm_lambda\": 25, \"irm_penalty_anneal_iters\": 500}\n",
            "\thparams_seed: 0\n",
            "\toutput_dir: ./results/irm_stronger_penalty\n",
            "\tsave_model_every_checkpoint: False\n",
            "\tseed: 0\n",
            "\tskip_model_save: False\n",
            "\tsteps: None\n",
            "\ttask: domain_generalization\n",
            "\ttest_envs: [3]\n",
            "\ttrial_seed: 0\n",
            "\tuda_holdout_fraction: 0\n",
            "HParams:\n",
            "\tbatch_size: 32\n",
            "\tclass_balanced: False\n",
            "\tdata_augmentation: True\n",
            "\tdinov2: False\n",
            "\tfreeze_bn: False\n",
            "\tirm_lambda: 25\n",
            "\tirm_penalty_anneal_iters: 500\n",
            "\tlars: False\n",
            "\tlinear_steps: 500\n",
            "\tlr: 5e-05\n",
            "\tnonlinear_classifier: False\n",
            "\tprogress_bar: True\n",
            "\tresnet18: False\n",
            "\tresnet50_augmix: True\n",
            "\tresnet_dropout: 0.0\n",
            "\tvit: False\n",
            "\tvit_attn_tune: False\n",
            "\tvit_dropout: 0.0\n",
            "\tweight_decay: 0.0\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    \n",
            "0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3839375973 \n",
            "0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0937460820 \n",
            "0.9640024405  0.9144254279  0.9493603412  0.9059829060  0.9865269461  0.9610778443  0.6758905852  0.6904458599  14.371257485  0.1064490070  8.1390233040  0.0646506229  0.0021149186  600           0.0935602132 \n",
            "0.7602196461  0.7188264059  0.6476545842  0.6025641026  0.8136227545  0.7514970060  0.4624681934  0.4878980892  21.556886227  -2.136413150  8.1390233040  1.3651420475  -0.140062212  900           0.0938191263 \n",
            "0.6558877364  0.6332518337  0.7579957356  0.7564102564  0.7642215569  0.7604790419  0.2108778626  0.2229299363  28.742514970  -8.894750553  8.1390233040  2.1037873693  -0.439941516  1200          0.0946129910 \n",
            "0.5546064674  0.5476772616  0.7457356077  0.7094017094  0.8248502994  0.7904191617  0.3797709924  0.3898089172  35.928143712  -2.978647657  8.1390233040  1.8122240104  -0.191634869  1500          0.0942125257 \n",
            "0.5997559487  0.6552567237  0.7601279318  0.7606837607  0.7791916168  0.7485029940  0.2888040712  0.3171974522  43.113772455  -2.682592541  8.1390233040  1.8046597634  -0.179490092  1800          0.0946811040 \n",
            "0.7156802929  0.7261613692  0.8107675906  0.7948717949  0.8143712575  0.7934131737  0.4036259542  0.4216560510  50.299401197  -5.215950786  8.1390233040  1.8884030006  -0.284174150  2100          0.0964806835 \n",
            "0.6809029896  0.6503667482  0.6295309168  0.6217948718  0.7724550898  0.7544910180  0.5054071247  0.5324840764  57.485029940  -3.269423261  8.1390233040  1.7315724899  -0.200039825  2400          0.0944827835 \n",
            "0.7248322148  0.7017114914  0.5719616205  0.5769230769  0.8360778443  0.7844311377  0.3963104326  0.4127388535  64.670658682  -8.316758580  8.1390233040  2.0203459839  -0.413484186  2700          0.0943294334 \n",
            "0.6931055522  0.6772616137  0.7563965885  0.7371794872  0.7911676647  0.7215568862  0.0814249364  0.0866242038  71.856287425  -8.551333123  8.1390233040  2.1629035880  -0.428569471  3000          0.0949624507 \n",
            "0.6113483832  0.6234718826  0.7062899787  0.6581196581  0.7133233533  0.6706586826  0.2181933842  0.2471337580  79.041916167  -2.360757904  8.1390233040  2.0249965721  -0.175430179  3300          0.0952030373 \n",
            "0.5649786455  0.5770171149  0.9019189765  0.8589743590  0.6856287425  0.6377245509  0.3730916031  0.3681528662  86.227544910  -7.900714660  8.1390233040  2.0224661835  -0.396927229  3600          0.0947308079 \n",
            "0.6284319707  0.6234718826  0.7547974414  0.7478632479  0.7791916168  0.7514970060  0.3594147583  0.3363057325  93.413173652  -8.267014676  8.1390233040  2.2515900618  -0.420744191  3900          0.0945808800 \n",
            "0.6717510677  0.6552567237  0.7052238806  0.7008547009  0.7896706587  0.7514970060  0.3571882952  0.3541401274  100.59880239  -13.29931690  8.1390233040  2.4695181207  -0.630753397  4200          0.0947346814 \n",
            "0.7169005491  0.6577017115  0.5026652452  0.4615384615  0.8555389222  0.7694610778  0.2350508906  0.2585987261  107.78443113  -20.43074037  8.1390233040  2.6539698178  -0.923388410  4500          0.0946750855 \n",
            "0.7303233679  0.7041564792  0.6609808102  0.6495726496  0.8884730539  0.8173652695  0.3311068702  0.3464968153  114.97005988  -19.82959846  8.1390233040  2.5842687408  -0.896554688  4800          0.0946627935 \n",
            "0.5698596705  0.5574572127  0.5021321962  0.4957264957  0.7252994012  0.6766467066  0.1962468193  0.1961783439  119.76047904  -12.43237737  8.1390233040  2.6434470254  -0.603032976  5000          0.0946760094 \n",
            "\n",
            "üéâ Training finished for IRM.\n"
          ]
        }
      ],
      "source": [
        "# --- IRM Experiment Configuration (Stronger Penalty) ---\n",
        "\n",
        "# 1. Define hyperparameters with the increased penalty.\n",
        "hparams_irm_stronger = {\n",
        "    \"progress_bar\": True,\n",
        "    \"irm_lambda\": 25,\n",
        "    \"irm_penalty_anneal_iters\": 500,\n",
        "}\n",
        "\n",
        "# 2. Configure the main experiment arguments.\n",
        "irm_args_stronger = {\n",
        "    \"data_dir\": \"./data/\",\n",
        "    \"dataset\": \"PACS\",\n",
        "    \"algorithm\": \"IRM\",\n",
        "    \"test_env\": 3,  # Sketch domain\n",
        "    \"output_dir\": \"./results/irm_stronger_penalty\",\n",
        "    \"hparams_seed\": 0,\n",
        "    \"trial_seed\": 0,\n",
        "    \"seed\": 0,\n",
        "    \"hparams\": f\"'{json.dumps(hparams_irm_stronger)}'\",\n",
        "}\n",
        "\n",
        "run_experiment(irm_args_stronger)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3788731",
      "metadata": {},
      "source": [
        "### **2.4. Ablation Study: Run IRM with Even Stronger Penalty**\n",
        "\n",
        "To complete our hyperparameter sensitivity analysis, we now test IRM with an even stronger penalty (`irm_lambda=100`), which is 10x stronger than the default. This creates a symmetric ablation study spanning three orders of magnitude: Œª=10 (default), Œª=25 (stronger), and Œª=100 (strongest)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc4e64a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Executing Command:\n",
            "PYTHONPATH=\"/root/IbsATML/PA2/Domain Generalisation/code/domainbed\" python -m domainbed.scripts.train --data_dir ./data/ --dataset PACS --algorithm IRM --test_env 3 --output_dir ./results/irm_weak_penalty --hparams_seed 0 --trial_seed 0 --seed 0 --hparams '{\"progress_bar\": true, \"irm_lambda\": 100, \"irm_penalty_anneal_iters\": 500}'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/venv/main/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment:\n",
            "\tPython: 3.12.11\n",
            "\tPyTorch: 2.8.0+cu129\n",
            "\tTorchvision: 0.23.0+cu129\n",
            "\tCUDA: 12.9\n",
            "\tCUDNN: 91002\n",
            "\tNumPy: 2.1.2\n",
            "\tPIL: 11.0.0\n",
            "Args:\n",
            "\talgorithm: IRM\n",
            "\tcheckpoint_freq: None\n",
            "\tdata_dir: ./data/\n",
            "\tdataset: PACS\n",
            "\tholdout_fraction: 0.2\n",
            "\thparams: {\"progress_bar\": true, \"irm_lambda\": 100, \"irm_penalty_anneal_iters\": 500}\n",
            "\thparams_seed: 0\n",
            "\toutput_dir: ./results/irm_weak_penalty\n",
            "\tsave_model_every_checkpoint: False\n",
            "\tseed: 0\n",
            "\tskip_model_save: False\n",
            "\tsteps: None\n",
            "\ttask: domain_generalization\n",
            "\ttest_envs: [3]\n",
            "\ttrial_seed: 0\n",
            "\tuda_holdout_fraction: 0\n",
            "HParams:\n",
            "\tbatch_size: 32\n",
            "\tclass_balanced: False\n",
            "\tdata_augmentation: True\n",
            "\tdinov2: False\n",
            "\tfreeze_bn: False\n",
            "\tirm_lambda: 100\n",
            "\tirm_penalty_anneal_iters: 500\n",
            "\tlars: False\n",
            "\tlinear_steps: 500\n",
            "\tlr: 5e-05\n",
            "\tnonlinear_classifier: False\n",
            "\tprogress_bar: True\n",
            "\tresnet18: False\n",
            "\tresnet50_augmix: True\n",
            "\tresnet_dropout: 0.0\n",
            "\tvit: False\n",
            "\tvit_attn_tune: False\n",
            "\tvit_dropout: 0.0\n",
            "\tweight_decay: 0.0\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           penalty       step          step_time    \n",
            "0.1122635754  0.1173594132  0.1897654584  0.2179487179  0.2080838323  0.2095808383  0.0890585242  0.1070063694  0.0000000000  1.9823751450  7.9325442314  1.9803798199  0.0019953740  0             0.3627316952 \n",
            "0.9890176937  0.9633251834  0.9888059701  0.9572649573  0.9992514970  0.9910179641  0.7283715013  0.7464968153  7.1856287425  0.2715274501  8.1374363899  0.2500528763  0.0214745743  300           0.0941438095 \n",
            "0.8364856620  0.8019559902  0.7995735608  0.7606837607  0.9371257485  0.9041916168  0.6103689567  0.6012738854  14.371257485  0.4529195653  8.1390233040  0.0822901678  0.0041631206  600           0.0936370540 \n",
            "0.7211714460  0.7261613692  0.7910447761  0.7884615385  0.7986526946  0.7305389222  0.6332697201  0.6433121019  21.556886227  -13.96678027  8.1390233040  1.4180853146  -0.153848656  900           0.0940670045 \n",
            "0.5216595485  0.5501222494  0.8363539446  0.8012820513  0.4738023952  0.5000000000  0.4875954198  0.4522292994  28.742514970  -56.68304686  8.1390233040  2.2477394044  -0.589307869  1200          0.0940271846 \n",
            "0.5454545455  0.5745721271  0.8256929638  0.8183760684  0.7507485030  0.7155688623  0.2569974555  0.2407643312  35.928143712  -59.33020267  8.1390233040  2.3291109520  -0.616593136  1500          0.0940203913 \n",
            "0.4612568639  0.5061124694  0.8448827292  0.8055555556  0.5179640719  0.5179640719  0.2544529262  0.2343949045  43.113772455  -36.47712758  8.1390233040  2.0347538857  -0.385118812  1800          0.0960043454 \n",
            "0.4972544234  0.5476772616  0.6961620469  0.6346153846  0.5449101796  0.5419161677  0.4688295165  0.4687898089  50.299401197  -28.88329039  8.1390233040  1.9190698512  -0.308023600  2100          0.0946115430 \n",
            "0.3978035387  0.4547677262  0.5991471215  0.5470085470  0.6002994012  0.5568862275  0.3428753181  0.3668789809  57.485029940  -14.41773042  8.1390233040  2.0186487929  -0.164363799  2400          0.0943737261 \n",
            "0.7126296522  0.7041564792  0.7622601279  0.7136752137  0.8570359281  0.7964071856  0.3823155216  0.3643312102  64.670658682  -44.18030029  8.1390233040  2.2926832441  -0.464729836  2700          0.0958478904 \n",
            "0.7071384991  0.7555012225  0.8379530917  0.8183760684  0.8555389222  0.8143712575  0.5314885496  0.5426751592  71.856287425  -49.65922636  8.1390233040  2.1198617830  -0.517790884  3000          0.0953671575 \n",
            "0.6388041489  0.6650366748  0.8267590618  0.7756410256  0.8278443114  0.7694610778  0.5372137405  0.5554140127  79.041916167  -74.29339531  8.1390233040  2.5307419417  -0.768241375  3300          0.0949552091 \n",
            "0.7547284930  0.7506112469  0.8678038380  0.8205128205  0.8488023952  0.8083832335  0.6491730280  0.6649681529  86.227544910  -95.51503682  8.1390233040  2.7619042653  -0.982769420  3600          0.0948590716 \n",
            "0.7504575961  0.7603911980  0.8336886994  0.8076923077  0.8697604790  0.8173652695  0.5524809160  0.5668789809  93.413173652  -70.30653072  8.1390233040  2.6472421871  -0.729537752  3900          0.0951091456 \n",
            "0.7022574741  0.7090464548  0.8267590618  0.7905982906  0.7252994012  0.6856287425  0.4131679389  0.4038216561  100.59880239  -50.12600257  8.1390233040  2.3383591020  -0.524643614  4200          0.0956722116 \n",
            "0.6119585113  0.6039119804  0.7777185501  0.7542735043  0.6953592814  0.6766467066  0.3721374046  0.3745222930  107.78443113  -35.00198434  8.1390233040  2.0799121298  -0.370818964  4500          0.0956371458 \n",
            "0.6308724832  0.6405867971  0.6050106610  0.5769230769  0.7275449102  0.6586826347  0.2057888041  0.2178343949  114.97005988  -2.207576399  8.1390233040  1.8208311947  -0.040284076  4800          0.0959791350 \n",
            "0.5704697987  0.5819070905  0.5831556503  0.5555555556  0.7694610778  0.7005988024  0.3183842239  0.3426751592  119.76047904  -49.89124073  8.1390233040  2.3570684242  -0.522483093  5000          0.0959641862 \n",
            "\n",
            "üéâ Training finished for IRM.\n"
          ]
        }
      ],
      "source": [
        "# --- IRM Experiment Configuration (Weaker Penalty) ---\n",
        "\n",
        "# 1. Define hyperparameters with an even more increased penalty.\n",
        "hparams_irm_weaker = {\n",
        "    \"progress_bar\": True,\n",
        "    \"irm_lambda\": 100,  # 10x stronger penalty\n",
        "    \"irm_penalty_anneal_iters\": 500,\n",
        "}\n",
        "\n",
        "# 2. Configure the main experiment arguments.\n",
        "irm_args_weaker = {\n",
        "    \"data_dir\": \"./data/\",\n",
        "    \"dataset\": \"PACS\",\n",
        "    \"algorithm\": \"IRM\",\n",
        "    \"test_env\": 3,  # Sketch domain\n",
        "    \"output_dir\": \"./results/irm_weak_penalty\",\n",
        "    \"hparams_seed\": 0,\n",
        "    \"trial_seed\": 0,\n",
        "    \"seed\": 0,\n",
        "    \"hparams\": f\"'{json.dumps(hparams_irm_weaker)}'\",\n",
        "}\n",
        "\n",
        "run_experiment(irm_args_weaker)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985dcd8f",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **Part 3: Group Distributionally Robust Optimization (GroupDRO)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68cd5267",
      "metadata": {},
      "source": [
        "### **3.1. Overview**\n",
        "\n",
        "Next, we implement **Group Distributionally Robust Optimization (GroupDRO)**. Instead of averaging the loss over all source domains like ERM, GroupDRO explicitly optimizes for the worst-case performance among them. At each step, it identifies the domain with the highest loss and updates the model to prioritize improving performance on this \"hardest\" domain. \n",
        "\n",
        "The goal is to prevent the model from simply overfitting to easier domains, thereby encouraging it to learn more robust features that can generalize better to unseen environments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3e02e9",
      "metadata": {},
      "source": [
        "### **3.2. Training GroupDRO**\n",
        "\n",
        "We will now train our model using the GroupDRO algorithm. The DomainBed library already includes this implementation. We only need to specify `GroupDRO` as the algorithm and set its associated hyperparameters. Based on the original paper and DomainBed's defaults, we will use a `groupdro_eta` of `1e-2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a3130e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting GroupDRO training...\n",
            "üöÄ Executing Command:\n",
            "PYTHONPATH=\"/root/IbsATML/PA2/Domain Generalisation/code/domainbed\" python -m domainbed.scripts.train --data_dir ./data/ --dataset PACS --algorithm GroupDRO --test_env 3 --output_dir ./results/groupdro --hparams_seed 0 --trial_seed 0 --seed 0 --hparams '{\"progress_bar\": true, \"groupdro_eta\": 0.01}'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/venv/main/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import parse_version\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment:\n",
            "\tPython: 3.12.11\n",
            "\tPyTorch: 2.8.0+cu129\n",
            "\tTorchvision: 0.23.0+cu129\n",
            "\tCUDA: 12.9\n",
            "\tCUDNN: 91002\n",
            "\tNumPy: 2.1.2\n",
            "\tPIL: 11.0.0\n",
            "Args:\n",
            "\talgorithm: GroupDRO\n",
            "\tcheckpoint_freq: None\n",
            "\tdata_dir: ./data/\n",
            "\tdataset: PACS\n",
            "\tholdout_fraction: 0.2\n",
            "\thparams: {\"progress_bar\": true, \"groupdro_eta\": 0.01}\n",
            "\thparams_seed: 0\n",
            "\toutput_dir: ./results/groupdro\n",
            "\tsave_model_every_checkpoint: False\n",
            "\tseed: 0\n",
            "\tskip_model_save: False\n",
            "\tsteps: None\n",
            "\ttask: domain_generalization\n",
            "\ttest_envs: [3]\n",
            "\ttrial_seed: 0\n",
            "\tuda_holdout_fraction: 0\n",
            "HParams:\n",
            "\tbatch_size: 32\n",
            "\tclass_balanced: False\n",
            "\tdata_augmentation: True\n",
            "\tdinov2: False\n",
            "\tfreeze_bn: False\n",
            "\tgroupdro_eta: 0.01\n",
            "\tlars: False\n",
            "\tlinear_steps: 500\n",
            "\tlr: 5e-05\n",
            "\tnonlinear_classifier: False\n",
            "\tprogress_bar: True\n",
            "\tresnet18: False\n",
            "\tresnet50_augmix: True\n",
            "\tresnet_dropout: 0.0\n",
            "\tvit: False\n",
            "\tvit_attn_tune: False\n",
            "\tvit_dropout: 0.0\n",
            "\tweight_decay: 0.0\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/venv/main/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    \n",
            "0.1183648566  0.1173594132  0.1849680171  0.2094017094  0.2088323353  0.2155688623  0.0798346056  0.0955414013  0.0000000000  1.9893321991  7.8673920631  0             0.3656907082 \n",
            "0.9780353874  0.9242053790  0.9728144989  0.9358974359  0.9977544910  0.9670658683  0.7442748092  0.7796178344  7.1856287425  0.2629900401  8.1327152252  300           0.0801037161 \n",
            "0.9920683344  0.9462102689  0.9722814499  0.9358974359  1.0000000000  0.9730538922  0.7604961832  0.7808917197  14.371257485  0.0433855407  8.1327152252  600           0.0801583258 \n",
            "0.9957291031  0.9486552567  0.9898720682  0.9444444444  0.9985029940  0.9670658683  0.8047073791  0.8229299363  21.556886227  0.0262665589  8.1327152252  900           0.0808297435 \n",
            "0.9957291031  0.9315403423  0.9957356077  0.9594017094  0.9992514970  0.9790419162  0.7560432570  0.7821656051  28.742514970  0.0195380715  8.1327152252  1200          0.0811634453 \n",
            "0.9938987187  0.9315403423  0.9893390192  0.9401709402  0.9977544910  0.9670658683  0.7779898219  0.8076433121  35.928143712  0.0203703395  8.1327152252  1500          0.0811521530 \n",
            "0.9951189750  0.9462102689  0.9941364606  0.9529914530  1.0000000000  0.9730538922  0.8031170483  0.8000000000  43.113772455  0.0161626841  8.1327152252  1800          0.0809528891 \n",
            "0.9945088469  0.9364303178  0.9957356077  0.9423076923  0.9985029940  0.9760479042  0.7773536896  0.7898089172  50.299401197  0.0123663371  8.1327152252  2100          0.0812310719 \n",
            "0.9975594875  0.9364303178  0.9978678038  0.9529914530  0.9992514970  0.9760479042  0.7433206107  0.7630573248  57.485029940  0.0111052407  8.1327152252  2400          0.0813440331 \n",
            "0.9969493594  0.9462102689  0.9930703625  0.9358974359  0.9992514970  0.9820359281  0.7903944020  0.8127388535  64.670658682  0.0119647305  8.1327152252  2700          0.0814694389 \n",
            "0.9963392312  0.9559902200  0.9984008529  0.9465811966  0.9992514970  0.9640718563  0.8053435115  0.8140127389  71.856287425  0.0091027992  8.1327152252  3000          0.0813256232 \n",
            "0.9993898719  0.9413202934  0.9968017058  0.9465811966  1.0000000000  0.9760479042  0.8082061069  0.8191082803  79.041916167  0.0122674725  8.1327152252  3300          0.0815166267 \n",
            "0.9975594875  0.9193154034  0.9973347548  0.9487179487  1.0000000000  0.9640718563  0.7827608142  0.8038216561  86.227544910  0.0058753829  8.1327152252  3600          0.0817226815 \n",
            "0.9957291031  0.9095354523  0.9909381663  0.9658119658  0.9985029940  0.9700598802  0.8005725191  0.8369426752  93.413173652  0.0075361112  8.1327152252  3900          0.0818663613 \n",
            "0.9969493594  0.9437652812  0.9989339019  0.9444444444  0.9977544910  0.9700598802  0.8142493639  0.8407643312  100.59880239  0.0092673628  8.1327152252  4200          0.0820469459 \n",
            "0.9969493594  0.9290953545  0.9968017058  0.9551282051  0.9977544910  0.9670658683  0.7830788804  0.8152866242  107.78443113  0.0059806193  8.1327152252  4500          0.0822232954 \n",
            "0.9975594875  0.9193154034  0.9962686567  0.9529914530  0.9992514970  0.9730538922  0.7986641221  0.8165605096  114.97005988  0.0100041138  8.1327152252  4800          0.0824292056 \n",
            "0.9963392312  0.9119804401  0.9946695096  0.9401709402  0.9992514970  0.9730538922  0.8346055980  0.8509554140  119.76047904  0.0082078094  8.1327152252  5000          0.0822734702 \n",
            "\n",
            "üéâ Training finished for GroupDRO.\n",
            "\n",
            "GroupDRO training complete!\n"
          ]
        }
      ],
      "source": [
        "# --- GroupDRO Experiment Configuration ---\n",
        "\n",
        "# 1. Define hyperparameters for GroupDRO\n",
        "hparams_groupdro = {\n",
        "    \"progress_bar\": True,\n",
        "    \"groupdro_eta\": 0.01  # Default learning rate for group weights\n",
        "}\n",
        "\n",
        "# 2. Configure the experiment arguments (same format as your IRM experiments)\n",
        "groupdro_args = {\n",
        "    \"data_dir\": \"./data/\",\n",
        "    \"dataset\": \"PACS\",\n",
        "    \"algorithm\": \"GroupDRO\",\n",
        "    \"test_env\": 3,  # Test on Sketch\n",
        "    \"output_dir\": \"./results/groupdro\",\n",
        "    \"hparams_seed\": 0,\n",
        "    \"trial_seed\": 0,\n",
        "    \"seed\": 0,\n",
        "    \"hparams\": f\"'{json.dumps(hparams_groupdro)}'\"\n",
        "}\n",
        "\n",
        "os.makedirs(groupdro_args['output_dir'], exist_ok=True)\n",
        "\n",
        "print(\"Starting GroupDRO training...\")\n",
        "run_experiment(groupdro_args)\n",
        "print(\"\\nGroupDRO training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb7dd8a1",
      "metadata": {},
      "source": [
        "### **3.3. Results & Analysis**\n",
        "\n",
        "After training is complete, we'll parse the output file to extract the final accuracies on both the source domains and the unseen target domain (Sketch) and do a quick comparison to our baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b20f69f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üìä GroupDRO Results Summary\n",
            "======================================================================\n",
            "\n",
            "üéØ Target Domain (Sketch): 85.10% (+1.28% vs ERM)\n",
            "\n",
            "üìö Source Domains:\n",
            "  ‚îú‚îÄ Art:     99.63%\n",
            "  ‚îú‚îÄ Cartoon: 99.47%\n",
            "  ‚îú‚îÄ Photo:   99.93%\n",
            "  ‚îî‚îÄ Average: 99.68%\n",
            "\n",
            "‚öñÔ∏è  Balance: Range=0.46% (ERM: 5.13%), Std=0.19%\n",
            "\n",
            "‚úÖ Status: OUTPERFORMED ERM!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load results\n",
        "try:\n",
        "    with open(os.path.join(groupdro_args['output_dir'], 'results.jsonl'), 'r') as f:\n",
        "        groupdro_results_log = [json.loads(line) for line in f]\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Results file for GroupDRO not found. Please ensure training completed successfully.\")\n",
        "    groupdro_results_log = []\n",
        "\n",
        "if groupdro_results_log:\n",
        "    groupdro_df = pd.DataFrame(groupdro_results_log)\n",
        "    \n",
        "    # Extract final accuracies\n",
        "    final_step = groupdro_df['step'].max()\n",
        "    final_accuracies = groupdro_df[groupdro_df['step'] == final_step]\n",
        "\n",
        "    # Get target and source accuracies\n",
        "    groupdro_target_acc = final_accuracies['env3_out_acc'].values[0] * 100\n",
        "    groupdro_art_acc = final_accuracies['env0_in_acc'].values[0] * 100\n",
        "    groupdro_cartoon_acc = final_accuracies['env1_in_acc'].values[0] * 100\n",
        "    groupdro_photo_acc = final_accuracies['env2_in_acc'].values[0] * 100\n",
        "    \n",
        "    # Calculate balance metrics\n",
        "    source_accuracies = [groupdro_art_acc, groupdro_cartoon_acc, groupdro_photo_acc]\n",
        "    source_avg = np.mean(source_accuracies)\n",
        "    source_range = max(source_accuracies) - min(source_accuracies)\n",
        "    source_std = np.std(source_accuracies)\n",
        "    \n",
        "    # Quick comparison to ERM\n",
        "    erm_target = 83.82\n",
        "    erm_range = 5.13  # 100.00 - 94.87\n",
        "    target_gap = groupdro_target_acc - erm_target\n",
        "    \n",
        "    # Display results\n",
        "    print(\"=\" * 70)\n",
        "    print(\"üìä GroupDRO Results Summary\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüéØ Target Domain (Sketch): {groupdro_target_acc:.2f}% ({target_gap:+.2f}% vs ERM)\")\n",
        "    print(f\"\\nüìö Source Domains:\")\n",
        "    print(f\"  ‚îú‚îÄ Art:     {groupdro_art_acc:.2f}%\")\n",
        "    print(f\"  ‚îú‚îÄ Cartoon: {groupdro_cartoon_acc:.2f}%\")\n",
        "    print(f\"  ‚îú‚îÄ Photo:   {groupdro_photo_acc:.2f}%\")\n",
        "    print(f\"  ‚îî‚îÄ Average: {source_avg:.2f}%\")\n",
        "    print(f\"\\n‚öñÔ∏è  Balance: Range={source_range:.2f}% (ERM: {erm_range:.2f}%), Std={source_std:.2f}%\")\n",
        "    \n",
        "    # Status check\n",
        "    if groupdro_target_acc > erm_target:\n",
        "        print(\"\\n‚úÖ Status: OUTPERFORMED ERM!\")\n",
        "    elif groupdro_target_acc > 78.85:\n",
        "        print(\"\\n‚ö†Ô∏è  Status: Better than IRM, below ERM\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Status: Underperformed\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to display. Training may have failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69736090",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Ibs-Kernel",
      "language": "python",
      "name": "ibs-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
